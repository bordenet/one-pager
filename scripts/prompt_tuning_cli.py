#!/usr/bin/env python3
"""
AI Agent Prompt Tuning Tool - Command Line Interface
"""
import os
import sys
import asyncio
import click
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel

# Add scripts directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from prompt_tuning_config import load_project_config, validate_api_keys, get_env_file_template
from prompt_simulator import PromptSimulator
from quality_evaluator import QualityEvaluator
from evolutionary_tuner import EvolutionaryTuner

console = Console()


@click.group()
@click.option('--verbose', '-v', is_flag=True, help='Enable verbose output')
@click.pass_context
def cli(ctx, verbose):
    """AI Agent Prompt Tuning Tool"""
    ctx.ensure_object(dict)
    ctx.obj['verbose'] = verbose


@cli.command()
@click.argument('project_name')
@click.option('--base-dir', type=click.Path(exists=True), help='Base directory (default: current)')
def init(project_name, base_dir):
    """Initialize a new prompt tuning project"""
    base_path = Path(base_dir) if base_dir else Path.cwd()

    console.print(f"[bold blue]Initializing prompt tuning project: {project_name}[/bold blue]")

    # Create directory structure
    results_dir = base_path / f"prompt_tuning_results_{project_name}"
    results_dir.mkdir(exist_ok=True)

    prompts_dir = base_path / "prompts"
    if not prompts_dir.exists():
        console.print(f"[yellow]Warning: prompts directory not found at {prompts_dir}[/yellow]")
        console.print("Please ensure your prompts are in the 'prompts/' directory")

    # Create .env template if it doesn't exist
    env_file = base_path / ".env"
    if not env_file.exists():
        with open(env_file, 'w') as f:
            f.write(get_env_file_template())
        console.print(f"[green]Created .env template at {env_file}[/green]")
        console.print("[yellow]Please fill in your API keys in the .env file[/yellow]")

    # Create sample test cases file
    test_cases_file = results_dir / f"test_cases_{project_name}.json"
    if not test_cases_file.exists():
        sample_test_cases = {
            "project": project_name,
            "description": "Test cases for prompt tuning",
            "test_cases": [
                {
                    "id": "tc001",
                    "name": "Sample Test Case",
                    "description": "A sample test case for demonstration",
                    "industry": "Technology",
                    "project_type": "Product Development",
                    "scope": "Medium",
                    "stakeholder_complexity": "High",
                    "inputs": {
                        "projectName": "Sample Project",
                        "problemDescription": "Sample problem description",
                        "businessContext": "Sample business context"
                    }
                }
            ]
        }

        import json
        with open(test_cases_file, 'w') as f:
            json.dump(sample_test_cases, f, indent=2)

        console.print(f"[green]Created sample test cases at {test_cases_file}[/green]")
        console.print("[yellow]Please customize the test cases for your project[/yellow]")

    console.print(f"[bold green]Project {project_name} initialized successfully![/bold green]")
    console.print(f"Results directory: {results_dir}")


@cli.command()
@click.argument('project_name')
@click.option('--iteration', '-i', default=0, help='Iteration number (0 for baseline)')
def simulate(project_name, iteration):
    """Run prompt simulation for test cases"""
    config = load_project_config(project_name)

    # Validate API keys
    missing_keys = validate_api_keys(config)
    if missing_keys:
        console.print("[bold red]Missing API keys:[/bold red]")
        for key in missing_keys:
            console.print(f"  - {key}")
        console.print("Please set these environment variables or update your .env file")
        sys.exit(1)

    console.print(f"[bold blue]Running simulation for {project_name} (iteration {iteration})[/bold blue]")

    async def run_simulation():
        simulator = PromptSimulator(config)

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("Running simulation...", total=None)
            results = await simulator.run_simulation(iteration)
            progress.update(task, description="Saving results...")
            output_file = simulator.save_results(results, iteration)

        console.print(f"[bold green]Simulation complete![/bold green]")
        console.print(f"Results saved to: {output_file}")

        return results

    return asyncio.run(run_simulation())


@cli.command()
@click.argument('project_name')
@click.option('--iteration', '-i', default=0, help='Iteration number (0 for baseline)')
def evaluate(project_name, iteration):
    """Evaluate simulation results"""
    config = load_project_config(project_name)

    console.print(f"[bold blue]Evaluating results for {project_name} (iteration {iteration})[/bold blue]")

    # Load simulation results
    if iteration == 0:
        results_file = config.results_dir / f"simulation_results_{project_name}_original.json"
    else:
        results_file = config.results_dir / f"simulation_results_{project_name}_iter{iteration}.json"

    if not results_file.exists():
        console.print(f"[bold red]Results file not found: {results_file}[/bold red]")
        console.print("Run 'simulate' command first")
        sys.exit(1)

    import json
    from prompt_simulator import SimulationResult

    with open(results_file, 'r') as f:
        data = json.load(f)

    # Convert to SimulationResult objects (simplified)
    results = []
    for result_data in data['results']:
        # This is a simplified conversion - in practice you'd need proper deserialization
        results.append(result_data)

    evaluator = QualityEvaluator(config)
    # Note: This would need proper object reconstruction in a full implementation
    console.print("[yellow]Evaluation feature needs full object reconstruction - see evolutionary_tuner for complete implementation[/yellow]")


@cli.command()
@click.argument('project_name')
def evolve(project_name):
    """Run evolutionary prompt tuning"""
    config = load_project_config(project_name)

    # Validate API keys
    missing_keys = validate_api_keys(config)
    if missing_keys:
        console.print("[bold red]Missing API keys:[/bold red]")
        for key in missing_keys:
            console.print(f"  - {key}")
        sys.exit(1)

    console.print(f"[bold blue]Starting evolutionary tuning for {project_name}[/bold blue]")

    async def run_evolution():
        tuner = EvolutionaryTuner(config)

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            task = progress.add_task("Running evolutionary tuning...", total=None)
            session = await tuner.run_evolutionary_tuning()

        # Display results
        console.print(Panel.fit(
            f"[bold green]Evolutionary Tuning Complete![/bold green]\n\n"
            f"Baseline Score: {session.baseline_score:.2f}/5.0\n"
            f"Final Score: {session.final_score:.2f}/5.0\n"
            f"Improvement: +{session.total_improvement:.2f} ({session.total_improvement/session.baseline_score*100:.1f}%)\n"
            f"Success Rate: {session.success_rate:.1%}\n"
            f"Iterations: {len(session.iterations)}",
            title="Results"
        ))

        return session

    return asyncio.run(run_evolution())


@cli.command()
@click.argument('project_name')
def status(project_name):
    """Show project status and results"""
    config = load_project_config(project_name)

    console.print(f"[bold blue]Status for project: {project_name}[/bold blue]")

    # Check directory structure
    table = Table(title="Project Structure")
    table.add_column("Component", style="cyan")
    table.add_column("Status", style="green")
    table.add_column("Path")

    # Check prompts directory
    if config.prompts_dir.exists():
        prompt_files = list(config.prompts_dir.glob("*.md"))
        table.add_row("Prompts", f"✅ {len(prompt_files)} files", str(config.prompts_dir))
    else:
        table.add_row("Prompts", "❌ Missing", str(config.prompts_dir))

    # Check results directory
    if config.results_dir.exists():
        result_files = list(config.results_dir.glob("*.json"))
        table.add_row("Results", f"✅ {len(result_files)} files", str(config.results_dir))
    else:
        table.add_row("Results", "❌ Missing", str(config.results_dir))

    # Check test cases
    if config.test_cases_file.exists():
        table.add_row("Test Cases", "✅ Found", str(config.test_cases_file))
    else:
        table.add_row("Test Cases", "❌ Missing", str(config.test_cases_file))

    console.print(table)

    # Check API keys
    missing_keys = validate_api_keys(config)
    if missing_keys:
        console.print("\n[bold red]Missing API Keys:[/bold red]")
        for key in missing_keys:
            console.print(f"  - {key}")
    else:
        console.print("\n[bold green]✅ All API keys configured[/bold green]")


if __name__ == '__main__':
    cli()
