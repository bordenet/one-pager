# Design Specification: Rigorous Evolutionary LLM Prompt Tuning System

**Version**: 2.0 - Evolutionary Methodology
**Date**: 2025-11-21
**Related**: GENERALIZED_PROMPT_TUNING_PRD.md

---

## ğŸ”¬ **CORE EVOLUTIONARY PRINCIPLES**

### Critical "Ah-Ha!" Moments Driving Design:

1. **Adversarial Tension Creates Value**: Phase 2 must generate DELIBERATELY DIFFERENT approaches, not incremental improvements
2. **Evolutionary Keep/Discard Logic**: Treat each change as a mutation - keep only if scores improve objectively
3. **Multi-Round Evolution**: 20-40 rounds of small mutations outperform single large changes
4. **Incremental Scoring**: Track every iteration to prevent regression and identify diminishing returns

---

## 1. Evolutionary System Architecture

### 1.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Rigorous Evolutionary Prompt Tuner              â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Project   â”‚  â”‚  Baseline   â”‚  â”‚  Mutation   â”‚        â”‚
â”‚  â”‚  Detector   â”‚  â”‚ Establisher â”‚  â”‚  Generator  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Simulation  â”‚  â”‚ Evolutionaryâ”‚  â”‚ Keep/Discardâ”‚        â”‚
â”‚  â”‚   Engine    â”‚  â”‚  Scorer     â”‚  â”‚   Logic     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Multi-Round Evolution Controller               â”‚
â”‚  - 20-round vs 40-round comparison                         â”‚
â”‚  - Mutation tracking and scoring                           â”‚
â”‚  - Diminishing returns detection                           â”‚
â”‚  - Optimal iteration count determination                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Evolutionary Components

1. **Baseline Establisher**: Creates unassailable baseline measurements with 8 diverse test cases
2. **Mutation Generator**: Designs specific, testable prompt changes targeting lowest-scoring criteria
3. **Evolutionary Scorer**: Applies standardized rubrics with objective evidence citation
4. **Keep/Discard Logic**: Makes binary decisions based on score improvements (no subjective judgment)
5. **Multi-Round Controller**: Manages 20-round and 40-round optimization cycles
6. **Diminishing Returns Detector**: Identifies when additional rounds stop adding value

### 1.3 Evolutionary Data Flow

```
Project Detection â†’ Baseline Establishment (8 test cases) â†’ Initial Scoring
     â†“
Round 1-20: Mutation â†’ Simulation â†’ Scoring â†’ Keep/Discard â†’ Documentation
     â†“
20-Round Analysis â†’ Diminishing Returns Assessment
     â†“
Round 21-40: Advanced Mutations â†’ Scoring â†’ Keep/Discard â†’ Documentation
     â†“
Comparative Analysis: 20-Round vs 40-Round Results â†’ Optimal Count Determination
```

## 2. Project Detection System

### 2.1 Supported Project Types

#### Type 1: one-pager (JavaScript/Markdown)
- **Prompt Location**: `prompts/*.md`
- **Format**: Markdown files with `{variable}` placeholders
- **Workflow**: 3-phase adversarial (phase1.md â†’ phase2.md â†’ phase3.md)
- **Output**: Business one-pager documents

#### Type 2: product-requirements-assistant (Go/Markdown)
- **Prompt Location**: `prompts/*.md`
- **Format**: Markdown files with `{variable}` placeholders
- **Workflow**: 3-phase collaborative (phase1 â†’ phase2 â†’ phase3)
- **Output**: Product Requirements Documents

#### Type 3: Generic Multi-Phase (Future)
- **Prompt Location**: `prompts/` or `templates/`
- **Format**: Markdown, JSON, or YAML
- **Workflow**: Configurable N-phase
- **Output**: Various document types

### 2.2 Detection Algorithm

```python
def detect_project_type(project_path):
    """Detect project type and prompt structure"""

    # Check for package.json (JavaScript)
    if exists(f"{project_path}/package.json"):
        if exists(f"{project_path}/prompts/phase1.md"):
            return ProjectType.ONE_PAGER

    # Check for go.mod (Go)
    if exists(f"{project_path}/go.mod") or exists(f"{project_path}/backend/go.mod"):
        if exists(f"{project_path}/prompts/phase1-claude-initial.md"):
            return ProjectType.PRODUCT_REQUIREMENTS_ASSISTANT

    # Generic detection
    prompt_files = find_files(project_path, ["*.md", "*.json", "*.yaml"], ["prompts", "templates"])
    if prompt_files:
        return ProjectType.GENERIC

    raise UnsupportedProjectError(f"No supported prompt structure found in {project_path}")
```

### 2.3 Prompt Structure Analysis

For each detected project:
1. **Parse prompt files** and identify variable placeholders
2. **Map workflow dependencies** (phase1 â†’ phase2 â†’ phase3)
3. **Extract expected output format** from prompt instructions
4. **Identify quality criteria** mentioned in prompts
5. **Document current structure** for baseline comparison

## 3. Test Generation System

### 3.1 Context-Aware Test Generation

The system generates test cases based on project context:

#### For one-pager Projects:
```python
def generate_onepager_tests(project_context):
    """Generate diverse one-pager test scenarios"""

    industries = ["SaaS", "E-commerce", "Healthcare", "Fintech", "Manufacturing"]
    project_types = ["New Feature", "Redesign", "Migration", "Optimization"]
    scopes = ["Small (2-4 weeks)", "Medium (2-3 months)", "Large (6+ months)"]

    test_cases = []
    for i in range(8):  # Generate 8 diverse cases
        test_case = {
            "id": f"test_{i+1:03d}",
            "industry": random.choice(industries),
            "project_type": random.choice(project_types),
            "scope": random.choice(scopes),
            "inputs": generate_realistic_inputs(industry, project_type, scope)
        }
        test_cases.append(test_case)

    return test_cases
```

#### For product-requirements-assistant Projects:
```python
def generate_prd_tests(project_context):
    """Generate diverse PRD test scenarios"""

    product_types = ["Mobile App", "Web Service", "API", "Platform", "Internal Tool"]
    complexities = ["Simple Feature", "Complex System", "Integration", "Migration"]
    audiences = ["B2B", "B2C", "Internal", "Partner-facing"]

    # Similar pattern with PRD-specific test generation
```

### 3.2 Test Case Structure

```json
{
  "project_type": "one-pager",
  "test_cases": [
    {
      "id": "test_001",
      "name": "Customer Portal Redesign",
      "context": {
        "industry": "SaaS",
        "project_type": "Redesign",
        "scope": "Medium (2-3 months)"
      },
      "inputs": {
        "projectName": "Customer Portal Redesign",
        "problemStatement": "Current portal has 45% bounce rate...",
        "proposedSolution": "Redesign with user-centered navigation...",
        "keyGoals": "Reduce bounce rate to <20%...",
        "scopeInScope": "Navigation redesign, search...",
        "scopeOutOfScope": "Backend API changes...",
        "successMetrics": "Bounce rate <20%, support tickets -40%...",
        "keyStakeholders": "Product Manager (Sarah Chen)...",
        "timelineEstimate": "12 weeks (2 weeks discovery...)"
      },
      "expected_qualities": {
        "clarity": 4,
        "conciseness": 5,
        "completeness": 4,
        "professionalism": 5,
        "actionability": 4
      }
    }
  ]
}
```

## 4. Simulation Engine

### 4.1 Mock LLM Response System

The simulation engine uses AI agent-generated responses that mimic real LLM behavior:

```python
class MockLLMEngine:
    """Generates realistic LLM responses for testing"""

    def __init__(self, project_type):
        self.project_type = project_type
        self.response_patterns = load_proven_patterns(project_type)

    def generate_response(self, prompt, phase, test_case):
        """Generate mock response based on proven patterns"""

        if self.project_type == "one-pager":
            return self._generate_onepager_response(prompt, phase, test_case)
        elif self.project_type == "product-requirements-assistant":
            return self._generate_prd_response(prompt, phase, test_case)

    def _generate_onepager_response(self, prompt, phase, test_case):
        """Generate one-pager mock response"""

        if phase == "phase1":
            # Business-focused, 550-650 words, strategic framing
            return self._create_phase1_onepager(test_case)
        elif phase == "phase2":
            # Adversarial alternative, different approach
            return self._create_phase2_onepager(test_case)
        elif phase == "phase3":
            # Synthesis of best elements, 600-700 words
            return self._create_phase3_onepager(test_case)
```

### 4.2 Multi-Phase Workflow Execution

```python
def execute_workflow(test_case, prompts):
    """Execute multi-phase workflow with proper dependencies"""

    results = {}

    # Phase 1: Initial generation
    phase1_prompt = substitute_variables(prompts["phase1"], test_case["inputs"])
    results["phase1"] = mock_llm.generate_response(phase1_prompt, "phase1", test_case)

    # Phase 2: Use Phase 1 output as input
    phase2_inputs = {**test_case["inputs"], "phase1_output": results["phase1"]}
    phase2_prompt = substitute_variables(prompts["phase2"], phase2_inputs)
    results["phase2"] = mock_llm.generate_response(phase2_prompt, "phase2", test_case)

    # Phase 3: Synthesis using both previous outputs
    phase3_inputs = {
        **test_case["inputs"],
        "phase1_output": results["phase1"],
        "phase2_output": results["phase2"]
    }
    phase3_prompt = substitute_variables(prompts["phase3"], phase3_inputs)
    results["phase3"] = mock_llm.generate_response(phase3_prompt, "phase3", test_case)

    return results
```

## 5. Quality Evaluation System

### 5.1 Universal Quality Rubric

```python
class QualityRubric:
    """Standardized quality evaluation system"""

    def __init__(self, project_type):
        self.project_type = project_type
        self.criteria = self._load_criteria(project_type)

    def _load_criteria(self, project_type):
        if project_type == "one-pager":
            return {
                "clarity": ClarityEvaluator(),
                "conciseness": ConcisenessEvaluator(),
                "completeness": CompletenessEvaluator(),
                "professionalism": ProfessionalismEvaluator(),
                "actionability": ActionabilityEvaluator()
            }
        elif project_type == "product-requirements-assistant":
            return {
                "comprehensiveness": ComprehensivenessEvaluator(),
                "clarity": ClarityEvaluator(),
                "structure": StructureEvaluator(),
                "consistency": ConsistencyEvaluator(),
                "engineering_ready": EngineeringReadyEvaluator(),
                "no_metadata_table": NoMetadataTableEvaluator(),
                "section_numbering": SectionNumberingEvaluator()
            }
```

### 5.2 Scoring Algorithm

Each criterion evaluator implements:
- **Score calculation** (1-5 scale or Pass/Fail)
- **Evidence extraction** (specific examples from output)
- **Improvement suggestions** (what would increase the score)

```python
class ClarityEvaluator:
    """Evaluates output clarity"""

    def evaluate(self, output, test_case):
        score = self._calculate_clarity_score(output)
        evidence = self._extract_clarity_evidence(output)
        suggestions = self._generate_clarity_suggestions(output, score)

        return EvaluationResult(
            criterion="clarity",
            score=score,
            evidence=evidence,
            suggestions=suggestions
        )
```

## 6. Rigorous Evolutionary Engine

### 6.1 Multi-Round Evolution Controller

```python
class EvolutionaryOptimizer:
    """Controls multi-round evolutionary optimization with objective scoring"""

    def __init__(self, project_type, baseline_results):
        self.project_type = project_type
        self.baseline_results = baseline_results
        self.baseline_score = calculate_average_score(baseline_results)
        self.current_prompts = load_current_prompts()
        self.current_score = self.baseline_score
        self.mutation_log = []
        self.round_number = 0

    def execute_20_round_optimization(self):
        """Execute 20 rounds of evolutionary optimization"""

        for round_num in range(1, 21):
            self.round_number = round_num

            # Identify target for mutation
            target_criterion = self._identify_lowest_scoring_criterion()

            # Generate specific mutation
            mutation = self._generate_mutation(target_criterion)

            # Apply mutation and test
            result = self._execute_mutation_round(mutation)

            # Log results
            self.mutation_log.append(result)

            # Check for diminishing returns
            if self._detect_diminishing_returns():
                break

        return self._generate_20_round_report()

    def execute_40_round_optimization(self):
        """Continue optimization for additional 20 rounds (total 40)"""

        for round_num in range(21, 41):
            self.round_number = round_num

            # Advanced mutation strategies for later rounds
            mutation = self._generate_advanced_mutation()
            result = self._execute_mutation_round(mutation)
            self.mutation_log.append(result)

        return self._generate_40_round_report()
```

### 6.2 Mutation Generation System

```python
def _generate_mutation(self, target_criterion):
    """Generate specific, testable mutation targeting lowest-scoring criterion"""

    mutation_strategies = {
        "clarity": [
            "add_explicit_examples",
            "strengthen_instruction_language",
            "add_definition_requirements"
        ],
        "conciseness": [
            "add_word_count_limits",
            "add_brevity_constraints",
            "require_bullet_points"
        ],
        "completeness": [
            "add_section_requirements",
            "mandate_specific_elements",
            "require_comprehensive_coverage"
        ],
        "professionalism": [
            "strengthen_business_language",
            "add_tone_requirements",
            "require_executive_framing"
        ],
        "actionability": [
            "require_specific_metrics",
            "mandate_next_steps",
            "add_timeline_requirements"
        ]
    }

    # Select specific mutation based on target criterion
    available_strategies = mutation_strategies[target_criterion]
    selected_strategy = choose_best_strategy(available_strategies, self.mutation_log)

    return create_specific_mutation(selected_strategy, target_criterion)

def create_specific_mutation(strategy, criterion):
    """Create specific, reversible prompt modification"""

    if strategy == "add_word_count_limits":
        return {
            "type": "word_count_enforcement",
            "target_files": ["phase1.md", "phase2.md", "phase3.md"],
            "modification": "Add explicit word count constraint: 'CRITICAL: Maximum 600 words.'",
            "expected_impact": f"Improve {criterion} scores by enforcing brevity",
            "reversible": True
        }

    elif strategy == "strengthen_business_language":
        return {
            "type": "business_framing",
            "target_files": ["phase1.md"],
            "modification": "Add requirement: 'Frame all content in business impact terms with quantifiable metrics.'",
            "expected_impact": f"Improve {criterion} through executive-level language",
            "reversible": True
        }

    # Additional mutation types...
```

### 6.3 Rigorous Keep/Discard Logic

```python
def _execute_mutation_round(self, mutation):
    """Execute single mutation round with rigorous scoring"""

    # Apply mutation to prompts
    mutated_prompts = apply_mutation(self.current_prompts, mutation)

    # Re-execute ALL test cases with mutated prompts
    new_results = simulate_all_test_cases(mutated_prompts, self.test_cases)

    # Score results using SAME rubric
    new_score = calculate_average_score(new_results)

    # Make objective keep/discard decision
    decision = self._make_keep_discard_decision(new_score)

    # Update state if keeping mutation
    if decision == Decision.KEEP:
        self.current_prompts = mutated_prompts
        self.current_score = new_score

    # Document round results
    return {
        "round": self.round_number,
        "mutation": mutation,
        "previous_score": self.current_score,
        "new_score": new_score,
        "delta": new_score - self.current_score,
        "decision": decision,
        "rationale": self._generate_decision_rationale(new_score),
        "test_case_impacts": calculate_per_test_case_impact(new_results)
    }

def _make_keep_discard_decision(self, new_score):
    """Objective binary decision based solely on score improvement"""

    if new_score > self.current_score:
        return Decision.KEEP
    else:
        return Decision.DISCARD

    # No subjective judgment allowed - purely objective scoring

def _detect_diminishing_returns(self):
    """Identify when additional rounds stop adding significant value"""

    if len(self.mutation_log) < 5:
        return False

    # Check last 5 rounds for improvement
    recent_improvements = [
        round_result["delta"] for round_result in self.mutation_log[-5:]
        if round_result["decision"] == Decision.KEEP
    ]

    # If no improvements in last 5 rounds, diminishing returns detected
    if len(recent_improvements) == 0:
        return True

    # If average improvement in last 5 rounds < 0.05, diminishing returns
    if sum(recent_improvements) / len(recent_improvements) < 0.05:
        return True

    return False
```

### 6.4 Comparative Analysis Engine

```python
def compare_20_vs_40_round_results(results_20, results_40):
    """Determine which approach produces superior results"""

    analysis = {
        "baseline_score": results_20["baseline_score"],
        "20_round_score": results_20["final_score"],
        "40_round_score": results_40["final_score"],
        "20_round_improvement": results_20["total_improvement"],
        "40_round_improvement": results_40["total_improvement"],
        "20_round_efficiency": results_20["improvement_per_round"],
        "40_round_efficiency": results_40["improvement_per_round"],
        "diminishing_returns_point": identify_diminishing_returns_point(results_40),
        "optimal_iteration_count": determine_optimal_count(results_20, results_40),
        "winner": determine_winner(results_20, results_40),
        "recommendation": generate_recommendation(results_20, results_40)
    }

    return analysis

def determine_winner(results_20, results_40):
    """Objective determination of which approach is superior"""

    # Primary criterion: Final quality score
    if results_40["final_score"] > results_20["final_score"] + 0.1:
        return "40-round (significantly better final quality)"

    elif results_20["final_score"] > results_40["final_score"] + 0.1:
        return "20-round (better efficiency, similar quality)"

    else:
        # Secondary criterion: Efficiency (improvement per round)
        if results_20["improvement_per_round"] > results_40["improvement_per_round"]:
            return "20-round (better efficiency, equivalent quality)"
        else:
            return "40-round (marginal quality advantage)"
```

## 7. AI Agent Execution Interface

### 7.1 Command-Line Interface

```python
# generalized_prompt_tuner.py
import click

@click.command()
@click.argument('project_path', type=click.Path(exists=True))
@click.option('--output-dir', default='./prompt_tuning_results', help='Output directory for results')
@click.option('--verbose', is_flag=True, help='Enable verbose logging')
def optimize(project_path, output_dir, verbose):
    """Optimize LLM prompts for any supported project"""

    # Phase 1: Project Detection
    click.echo("ğŸ” Detecting project type and prompt structure...")
    project_type = detect_project_type(project_path)
    click.echo(f"âœ… Detected: {project_type}")

    # Phase 2: Test Generation
    click.echo("ğŸ“ Generating test cases...")
    test_cases = generate_test_cases(project_type, project_path)
    click.echo(f"âœ… Generated {len(test_cases)} test cases")

    # Phase 3: Baseline Evaluation
    click.echo("âš¡ Running baseline simulations...")
    baseline_results = run_simulations(project_path, test_cases)
    baseline_score = evaluate_results(baseline_results, project_type)
    click.echo(f"ğŸ“Š Baseline score: {baseline_score:.2f}/5.0")

    # Phase 4: Optimization
    click.echo("ğŸ§¬ Optimizing prompts...")
    optimized_prompts = optimize_prompts(project_path, baseline_results)

    # Phase 5: Validation
    click.echo("âœ… Validating improvements...")
    improved_results = run_simulations_with_prompts(optimized_prompts, test_cases)
    improved_score = evaluate_results(improved_results, project_type)

    # Phase 6: Results
    improvement = improved_score - baseline_score
    click.echo(f"ğŸ¯ Final score: {improved_score:.2f}/5.0 (+{improvement:.2f})")

    if improvement >= 0.12:  # 12% improvement threshold
        click.echo("ğŸ‰ SUCCESS: Significant improvement achieved!")
        save_results(output_dir, baseline_results, improved_results, optimized_prompts)
    else:
        click.echo("âš ï¸  Minimal improvement. Consider manual review.")

if __name__ == '__main__':
    optimize()
```

### 7.2 Progress Tracking

```python
class ProgressTracker:
    """Track and display optimization progress"""

    def __init__(self):
        self.current_phase = None
        self.total_phases = 6
        self.current_step = 0
        self.total_steps = 0

    def start_phase(self, phase_name, total_steps):
        self.current_phase = phase_name
        self.total_steps = total_steps
        self.current_step = 0
        click.echo(f"ğŸ”„ {phase_name}...")

    def update_step(self, step_description):
        self.current_step += 1
        progress = (self.current_step / self.total_steps) * 100
        click.echo(f"   [{progress:5.1f}%] {step_description}")

    def complete_phase(self, result_summary):
        click.echo(f"âœ… {self.current_phase} complete: {result_summary}")
```

## 8. Report Generation System

### 8.1 Comprehensive Analysis Report

```python
def generate_analysis_report(project_path, baseline_results, improved_results, optimizations):
    """Generate comprehensive analysis report"""

    report = AnalysisReport()

    # Executive Summary
    report.add_section("Executive Summary", {
        "project_path": project_path,
        "baseline_score": calculate_average_score(baseline_results),
        "improved_score": calculate_average_score(improved_results),
        "improvement_delta": calculate_improvement_delta(baseline_results, improved_results),
        "optimizations_applied": len(optimizations),
        "success_threshold_met": improvement_delta >= 0.12
    })

    # Detailed Results
    report.add_section("Test Case Analysis",
        generate_test_case_analysis(baseline_results, improved_results))

    # Applied Optimizations
    report.add_section("Optimizations Applied",
        generate_optimization_summary(optimizations))

    # Before/After Comparisons
    report.add_section("Before/After Examples",
        generate_comparison_examples(baseline_results, improved_results))

    return report
```

### 8.2 Report Templates

```markdown
# Prompt Optimization Report: {project_name}

**Date**: {timestamp}
**Project Path**: {project_path}
**Project Type**: {project_type}

## Executive Summary

- **Baseline Score**: {baseline_score}/5.0
- **Improved Score**: {improved_score}/5.0
- **Improvement**: +{delta} ({percentage}%)
- **Success Threshold**: {threshold_met} (â‰¥12% required)

## Key Achievements

âœ… **Quality Improvements**:
- Clarity: {clarity_before} â†’ {clarity_after} (+{clarity_delta})
- Conciseness: {conciseness_before} â†’ {conciseness_after} (+{conciseness_delta})
- Completeness: {completeness_before} â†’ {completeness_after} (+{completeness_delta})

âœ… **Optimizations Applied**:
1. {optimization_1}: {description_1}
2. {optimization_2}: {description_2}
3. {optimization_3}: {description_3}

## Test Case Results

### Test Case: {test_case_name}
- **Before**: {before_score}/5.0
- **After**: {after_score}/5.0
- **Delta**: +{delta}

**Key Improvements**:
- {improvement_1}
- {improvement_2}

**Example Output Comparison**:

**Before**:
```
{before_excerpt}
```

**After**:
```
{after_excerpt}
```

## Recommendations for Future

- {recommendation_1}
- {recommendation_2}
- {recommendation_3}
```

## 9. Integration with Existing Tools

### 9.1 Comparison with codebase-reviewer Solution

**Similarities**:
- âœ… 7-phase workflow structure (Analysis â†’ Testing â†’ Optimization â†’ Validation)
- âœ… Quality rubric-based evaluation system
- âœ… Before/after comparison reporting
- âœ… AI agent autonomous execution capability

**Key Differences**:

| Aspect | codebase-reviewer | one-pager (This Solution) |
|--------|------------------|---------------------------|
| **Scope** | Single project focus | Generalized for any project |
| **Architecture** | Manual workflow steps | Automated Python tool |
| **Mock Responses** | Manual JSON creation | AI agent-generated responses |
| **Evolution** | Manual recommendations | Automated keep/discard logic |
| **Integration** | Standalone process | Genesis-ready deployment |
| **Execution** | Human-guided | Fully autonomous |

**Lessons Learned from codebase-reviewer**:
1. **Detailed rubrics are essential** - Specific 1-5 scoring with clear criteria
2. **Test case diversity matters** - Need variety in industries, scopes, complexities
3. **Before/after examples are powerful** - Concrete evidence of improvements
4. **Prioritization is critical** - Focus on high-impact changes first
5. **Validation prevents regressions** - Always check that improvements don't break other areas

**Improvements in This Solution**:
1. **Full automation** - No manual steps required
2. **Proven patterns** - Mock responses based on successful +12% improvements
3. **Universal applicability** - Works across different project types
4. **Genesis integration** - Ready for automatic deployment
5. **Evolutionary optimization** - Systematic keep/discard based on objective scoring

### 9.2 Genesis Integration Strategy

```python
# genesis/tools/prompt_tuning/
â”œâ”€â”€ generalized_prompt_tuner.py     # Main CLI tool
â”œâ”€â”€ project_detectors/              # Project type detection
â”‚   â”œâ”€â”€ onepager_detector.py
â”‚   â”œâ”€â”€ prd_assistant_detector.py
â”‚   â””â”€â”€ generic_detector.py
â”œâ”€â”€ test_generators/                # Context-aware test generation
â”‚   â”œâ”€â”€ onepager_tests.py
â”‚   â”œâ”€â”€ prd_tests.py
â”‚   â””â”€â”€ generic_tests.py
â”œâ”€â”€ mock_engines/                   # AI agent response simulation
â”‚   â”œâ”€â”€ onepager_mock.py
â”‚   â”œâ”€â”€ prd_mock.py
â”‚   â””â”€â”€ base_mock.py
â”œâ”€â”€ evaluators/                     # Quality assessment
â”‚   â”œâ”€â”€ universal_rubric.py
â”‚   â”œâ”€â”€ onepager_rubric.py
â”‚   â””â”€â”€ prd_rubric.py
â””â”€â”€ reports/                        # Analysis and comparison
    â”œâ”€â”€ report_generator.py
    â””â”€â”€ templates/
```

**Genesis Bootstrapping Integration**:
```bash
# During genesis project creation
genesis create my-new-project --template=onepager
cd my-new-project
python ../genesis/tools/prompt_tuning/generalized_prompt_tuner.py optimize .
# Automatically optimized prompts ready for use
```

## 10. Success Metrics and Validation

### 10.1 Quantitative Success Criteria

- **Quality Improvement**: â‰¥12% average score increase (proven benchmark)
- **Execution Time**: â‰¤30 minutes total runtime
- **Automation Rate**: 100% autonomous execution (no human intervention)
- **Consistency**: â‰¥90% of projects achieve target improvement
- **Reliability**: 100% success rate on supported project types

### 10.2 Validation Approach

1. **Baseline Validation**: Test on one-pager and product-requirements-assistant
2. **Cross-Project Validation**: Apply to 3-5 additional Genesis projects
3. **Regression Testing**: Ensure no degradation in quality over time
4. **Performance Benchmarking**: Measure execution time across project sizes
5. **User Acceptance**: AI agents can execute without guidance

## 11. Implementation Roadmap

### Phase 1: Core System (Week 1)
- [ ] Project detection for one-pager and product-requirements-assistant
- [ ] Test case generation with proven patterns
- [ ] Mock LLM response system with +12% improvement patterns
- [ ] Basic quality evaluation rubric

### Phase 2: Optimization Engine (Week 2)
- [ ] Improvement identification algorithms
- [ ] Prompt mutation system
- [ ] Keep/discard validation logic
- [ ] Before/after comparison reporting

### Phase 3: AI Agent Interface (Week 3)
- [ ] Command-line interface with progress tracking
- [ ] Autonomous execution mode
- [ ] Comprehensive report generation
- [ ] Error handling and edge cases

### Phase 4: Genesis Integration (Week 4)
- [ ] Genesis project structure integration
- [ ] Bootstrapping process automation
- [ ] Documentation and usage guides
- [ ] Production deployment and validation

## 12. Risk Mitigation

### 12.1 Technical Risks

**Risk**: Mock responses don't accurately represent real LLM behavior
- **Mitigation**: Base mocks on proven patterns from successful manual tuning
- **Validation**: Compare mock results with actual LLM outputs on sample cases

**Risk**: Project-specific edge cases cause failures
- **Mitigation**: Start with known project types, expand gradually
- **Validation**: Comprehensive testing on diverse project structures

**Risk**: Quality improvements not consistent across different project types
- **Mitigation**: Standardized rubric with project-specific adaptations
- **Validation**: Cross-project validation with consistent success metrics

### 12.2 Operational Risks

**Risk**: AI agents cannot execute autonomously without human guidance
- **Mitigation**: Comprehensive documentation and clear success criteria
- **Validation**: Test autonomous execution in isolated environment

**Risk**: Integration with Genesis breaks existing workflows
- **Mitigation**: Optional integration, backward compatibility maintained
- **Validation**: Parallel testing with existing Genesis projects

## 13. Next Steps

1. âœ… **PRD and Design Approval**: Review and approve this specification
2. â³ **Core Implementation**: Build project detection and test generation
3. â³ **Mock Engine Development**: Create AI agent response simulation
4. â³ **Validation Testing**: Test on one-pager and product-requirements-assistant
5. â³ **Genesis Integration**: Embed into Genesis project structure
6. â³ **Production Deployment**: Make available for all Genesis projects

---

**This design specification provides the foundation for a generalized, AI-agent-executable prompt tuning system that can achieve consistent +12% quality improvements across any supported project type in â‰¤30 minutes of autonomous execution.**
```
