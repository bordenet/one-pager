# AI Agent Prompt Tuning Tool

**Automated LLM prompt optimization with evolutionary mutation-based tuning**

This tool implements the successful methodology developed during the one-pager project prompt tuning, providing automated optimization of LLM prompts with keep/discard mutation logic.

## ğŸ¯ What This Tool Does

1. **Simulates LLM workflows** with real API calls to Claude, Gemini, and GPT
2. **Evaluates quality** using 5-point rubric (Clarity, Conciseness, Impact, Feasibility, Completeness)
3. **Applies mutations** based on recommendations (word count enforcement, strategic framing, etc.)
4. **Keeps improvements, discards regressions** using objective scoring
5. **Stops automatically** when targets are met or diminishing returns detected

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements-prompt-tuning.txt
```

### 2. Set Up API Keys

Copy `.env.example` to `.env` and fill in your API keys:

```bash
cp .env.example .env
# Edit .env with your actual API keys
```

### 3. Initialize Project

```bash
python prompt_tuning_tool.py init my-project
```

This creates:
- `prompt_tuning_results_my-project/` directory
- Sample test cases file
- `.env` template (if needed)

### 4. Customize Test Cases

Edit `prompt_tuning_results_my-project/test_cases_my-project.json`:

```json
{
  "project": "my-project",
  "test_cases": [
    {
      "id": "tc001",
      "name": "High-Stakes Enterprise Project",
      "industry": "Technology",
      "project_type": "System Migration",
      "inputs": {
        "projectName": "Legacy System Migration",
        "problemDescription": "Critical system failing, impacting revenue",
        "businessContext": "Enterprise customer retention at risk"
      }
    }
  ]
}
```

### 5. Run Evolutionary Tuning

```bash
python prompt_tuning_tool.py evolve my-project
```

This will:
- Run baseline simulation
- Apply mutations one by one
- Keep improvements, discard regressions
- Generate comprehensive reports

## ğŸ“ Project Structure

```
your-project/
â”œâ”€â”€ prompts/                    # Original prompts
â”‚   â”œâ”€â”€ phase1.md              # Initial draft prompt
â”‚   â”œâ”€â”€ phase2.md              # Adversarial review prompt
â”‚   â””â”€â”€ phase3.md              # Synthesis prompt
â”œâ”€â”€ prompt_tuning_results_my-project/
â”‚   â”œâ”€â”€ test_cases_my-project.json
â”‚   â”œâ”€â”€ simulation_results_*.json
â”‚   â”œâ”€â”€ evaluation_report_*.md
â”‚   â”œâ”€â”€ evolutionary_session.json
â”‚   â””â”€â”€ phase*_iter*.md        # Evolved prompts
â””â”€â”€ .env                       # API keys
```

## ğŸ”§ Commands

### Initialize New Project
```bash
python prompt_tuning_tool.py init <project_name>
```

### Check Project Status
```bash
python prompt_tuning_tool.py status <project_name>
```

### Run Simulation Only
```bash
python prompt_tuning_tool.py simulate <project_name> [--iteration N]
```

### Run Full Evolutionary Tuning
```bash
python prompt_tuning_tool.py evolve <project_name>
```

## ğŸ“Š Understanding Results

### Evolutionary Session Report
- **Baseline Score**: Starting quality score
- **Final Score**: Ending quality score after evolution
- **Total Improvement**: Net improvement achieved
- **Success Rate**: Percentage of mutations that were kept
- **Iterations**: Number of mutations tested

### Quality Scoring (1-5 scale)
- **Clarity**: Structure, readability, specific metrics
- **Conciseness**: Word count within target range (500-700)
- **Impact**: Business framing, strategic positioning, urgency
- **Feasibility**: Timeline, phases, risks, resources
- **Completeness**: Required sections, substantive content

### Mutation Logic
- **KEEP**: New score > baseline score (any improvement)
- **DISCARD**: New score â‰¤ baseline score (no improvement/regression)

## ğŸ›ï¸ Configuration

### API Providers
- **Anthropic Claude**: `claude-3-5-sonnet-20241022`
- **Google Gemini**: `gemini-1.5-pro`
- **OpenAI GPT**: `gpt-4` (optional)

### Stopping Criteria
- Target score achieved (â‰¥4.0) with sufficient iterations
- Stretch goal achieved (â‰¥4.5)
- Max iterations reached (20)
- Diminishing returns detected (<0.05 improvement for 3 iterations)

### Customization
Edit `scripts/prompt_tuning_config.py` to modify:
- Scoring criteria and weights
- Word count targets
- Evolution parameters
- LLM model configurations

## ğŸ§ª Testing

```bash
# Test individual components
python -m scripts.prompt_simulator my-project
python -m scripts.evolutionary_tuner my-project

# Test API connections
python -c "from scripts.llm_client import test_llm_connection; from scripts.prompt_tuning_config import DEFAULT_PHASE_CONFIGS; import asyncio; print(asyncio.run(test_llm_connection(DEFAULT_PHASE_CONFIGS['phase1'])))"
```

## ğŸ“ˆ Success Metrics

Based on one-pager project results:
- **Overall improvement**: +12% quality score
- **Phase 1 improvement**: +24% (3.56 â†’ 4.40)
- **Phase 3 improvement**: +14% (4.16 â†’ 4.76)
- **Word count compliance**: 90% â†’ 100%

## ğŸ”„ Applying to New Projects

1. **Copy your prompts** to `prompts/` directory
2. **Create test cases** representing your domain
3. **Run evolutionary tuning** with `evolve` command
4. **Deploy improved prompts** from results directory
5. **Document lessons learned** for next project

## ğŸ› ï¸ Troubleshooting

### Missing API Keys
```
Error: Missing API keys: phase1: ANTHROPIC_API_KEY
```
**Solution**: Set environment variables or update `.env` file

### No Improvements Found
```
Stopping: All recommendations tested
```
**Solution**: Add more recommendations to `recommendations_*.json` or adjust mutation logic

### Rate Limiting
**Solution**: Add delays between API calls in `llm_client.py`

## ğŸ“š Based On

This tool implements the methodology developed during the one-pager project prompt tuning, documented in:
- `prompt_tuning_results_one-pager/FINAL_REPORT.md`
- `prompt_tuning_results_one-pager/DESIGN_EVOLUTIONARY_PROMPT_TUNING.md`
- `prompt_tuning_results_one-pager/PRD_PRODUCT_REQUIREMENTS_ASSISTANT.md`

The approach achieved significant quality improvements and is now automated for reuse across projects.
